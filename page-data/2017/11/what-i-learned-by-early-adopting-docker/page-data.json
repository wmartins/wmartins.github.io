{"componentChunkName":"component---src-templates-blog-post-js","path":"/2017/11/what-i-learned-by-early-adopting-docker/","result":{"data":{"markdownRemark":{"html":"<p>The year is 2015, I started working as software engineer in this new team that\nwas focused on delivering new experiences with top notch technology. This was\nalso an year when microservices began to be a thing, and, with that, Docker\nbeing the <em>de facto</em> containerization tool.</p>\n<p>At this time, Docker was on release <code class=\"language-text\">1.6</code> or <code class=\"language-text\">1.7</code>, and few people were using it\nto deploy applications. Lots of those were POCs, and most of the blog posts\nincluded some \"strange\" things (not the Netflix title) there, like the <code class=\"language-text\">fig</code>\ncommand (which became <code class=\"language-text\">docker-compose</code>), and lots of lines in <code class=\"language-text\">yml</code> code.</p>\n<p>It was a time where most of the \"real world problems\" were still being found for\nDocker deployments. Things like networking, orchestration, high availability,\nand so on. So, we made a few bad decisions that made us learn a lot (at least\nwhat not to do), but that we would like someone had told us that were bad ideas.</p>\n<p>This blog post will cover some of those bad decisions, explaining why they're\nbad, and what we should've done.</p>\n<h2>Outlining the Application</h2>\n<p>Our application, in a high level, was designed to be high available, which means\nthat, at least, we needed to run our application in more than one machine. That\nsaid, we needed to be able to run replicas of containers, and provide a\nmechanism to use those replicas in the best way we can.</p>\n<p>In our case, we decided to use <a href=\"https://openresty.org/en/\"><strong>OpenResty</strong></a> (Nginx</p>\n<ul>\n<li>Lua Scripts) as a reverse proxy/API gateway/load balancer, balancing the\ntraffic between the containers. Something like that:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"conf\"><pre class=\"language-conf\"><code class=\"language-conf\">upstream first-microservice {\n    server &lt;FIRST-SERVER-IP&gt;:8877;\n    server &lt;SECOND-SERVER-IP&gt;:8877;\n}</code></pre></div>\n<p>After we decided what goes where, we then took two bad decisions: orchestrate\nthe application using <a href=\"https://jenkins.io/\"><strong>Jenkins</strong></a> and creating a\n<code class=\"language-text\">configuration</code> volume, which I'll cover in next sections.</p>\n<h2>Orchestrating Using Jenkins</h2>\n<p><strong>Jenkins</strong> is an awesome tool. It does its job really well:</p>\n<blockquote>\n<p>The leading open source <strong>automation</strong> server, Jenkins provides hundreds of\nplugins to support building, deploying and automating any project.</p>\n</blockquote>\n<p>As you see, it's an <strong>automation</strong> server. Putting it in another way, it's\n<strong>not</strong> an orchestration server. But, for some reason, we thought it was.</p>\n<p>So, our deployment jobs now had the IPs of all our machines, containing a Groovy\nscript to associate an IP with container names (in order to know what to\ndeploy). Then, a generic deploy job just grabbed the IP, the names, ssh'd into\nthe machines and <em>voila</em>, let <code class=\"language-text\">docker-compose up -d &lt;CONTAINERS&gt;</code> do its job.</p>\n<p>It seems good, right? It felt good. It really did, but it started to become hard\nto manage. Every new microservice we added, we needed to edit this Jenkins job\nand add the new microservice in <strong>all</strong> environments (we have 2 QA environments,\n1 <em>production like</em> environment and 1 production environment). And, there was <strong>no\nspace for mistakes</strong> here. If you edit it wrong, your service will go to the\nwrong machine, and the application might not work. Also, if you put, for\nexample, two applications that consume lots of memory in the same machine, you\nmight compromise this machine, just because you chose the wrong place to put\nyour microservice.</p>\n<p>So, as I said before, we found out that Jenkins is not an orchestration tool.\nIt doesn't handle unhealthy services/machines, it doesn't handle dynamic\nadding/removing replicas, it doesn't monitor your replicas. It just automates\nstuff.</p>\n<p>Now, the plan is to move this responsibility from Jenkins to <strong>Docker Swarm</strong>,\nwhich will also gives us other cool features that fix more of our problems.</p>\n<p>There are other tools, like <strong>Kubernetes</strong> and <strong>Nomad</strong>, that also solves this\nin a fashion way.</p>\n<h2>No Service Discovery/Hardcoded IPs</h2>\n<p>Service discovery is one of the most important aspects of a successful\nmicroservices architecture. Without it, you'll struggle when adding/removing\nreplicas from the application.</p>\n<p>We found out that after having some problems on some of our machines (our\ninfrastructure team had an outage on a datacenter, which took half of our\nmachines down). In that time, we wanted to add more replicas to make the\napplication high available again. It was really hard to do:</p>\n<ol>\n<li>Editing IPs in Jenkins and config files</li>\n<li>Going in Jenkins jobs to modify where to deploy stuff</li>\n<li>Deploy and check if everything is running fine</li>\n</ol>\n<p>Then, after the machines went up again, we needed to do the reverse process,\nediting files, modifying Jenkins and deploying, all over again.</p>\n<p>It's not that hard to setup a <code class=\"language-text\">DNS</code> or a service discovery system. So, don't let\nthe \"lets ship it soon\" make you do things that will only cause you trouble. Or,\nif you do, at least know what you need to do to make it right.</p>\n<h2>Configuration as a Shared Volume</h2>\n<p>All applications, at some point, need some kind of configuration. Most likely,\nthe application, when in production, will need to behave differently than in the\nlocal machine, or in a low level (testing) environment. This requires us to have\nsome level of <strong>configuration management</strong>.</p>\n<p>Back at that time, the way we solved it was to create a <strong>configuration\nvolume</strong>, that is a simple Docker container that we use to store configuration\nfiles per environment. Then, at deployment time, we just grab the correct\nconfiguration container and we deploy it using <code class=\"language-text\">--volumes-from</code> directive.</p>\n<p>It seemed to be a good idea, but we started realizing that it doesn't scale.\nFirst of all, we were coupling all microservices with this configuration\ncontainer, and we needed to always remember to edit the files in it. Second, our\nmicroservices weren't really standalone, as we couldn't run it without running\nthis other container together. Third, if two containers used the version\n<code class=\"language-text\">2.1.0</code> of the configuration container, they would end up using exactly the same\nfiles. Putting it in another way, if, by mistake, one container edits the wrong\nfile, it could affect the other container. To finish, it starts to be really\nhard to manage which is the correct configuration version to run with each\nmicroservice.</p>\n<p>There are some ways to do it correctly. One of the easiest is to use\n<strong>environment variables</strong> for all those configuration objects. If the\napplication requires a string to connect to the database, provide a\n<code class=\"language-text\">DB_CONNECTION_STRING</code> env var and so on.</p>\n<p>The other way of using it is to use a <strong>configuration management tool</strong> that let\nyou externalize your configuration. Tools like <strong>Consul</strong> with its key-value\nstorage solve that for you. The only downside is that you'll need to modify your\napplications to connect to that tool to grab the values. The cool thing about\nit is that you can, in the middle of the day, change some configuration and,\nif your code supports it, your application will notice that and will adapt\nitself. It's really nice.</p>\n<h2>Wrapping Up</h2>\n<p>It was really hard to early adopt Docker. We made a few poor decisions that\nwe're not proud of. Also, those decisions made our jobs pretty hard. If, at that\ntime, we discussed more and researched more, we would probably doing things\nbetter.</p>\n<p>That said, my advice if you're adopting Docker (but that can be used to\nanything) is to study a lot, discuss in forums and experiment with it. Try to\nimplement all your use cases and think what you'll need when running in\nproduction, and check if you have the tools and knowledge to make you go\ncomfortably to production.</p>\n<p>Hope it was useful!</p>","frontmatter":{"title":"What I Learned by Early Adopting Docker","tags":["docker","orchestration"]}}},"pageContext":{"slug":"/2017/11/what-i-learned-by-early-adopting-docker/"}}}